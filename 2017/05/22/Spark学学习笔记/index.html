<!doctype html>



  


<html class="theme-next pisces use-motion" lang="zh-Hans">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>






<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />




  
  
  
  

  
    
    
  

  

  

  

  

  
    
    
    <link href="//fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css">
  






<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.1" rel="stylesheet" type="text/css" />


  <meta name="keywords" content="Hexo, NexT" />








  <link rel="shortcut icon" type="image/x-icon" href="/favicon.ico?v=5.1.1" />






<meta name="description" content="注：根据网站https://databricks.com/整理  spark核心共有4个模块，面对的实际情景如下图所示 RDD(Resilient Distribute Datasets)目前，我对spark的理解浅显的停留在一种是大数据处理的框架。那么，要处理数据，就必须了解其中重要的操作元素RDD的概念。RDD在官网的定义是：It is a fault-tolerant collection">
<meta property="og:type" content="article">
<meta property="og:title" content="Spark学习笔记（1）术语概览(updating)">
<meta property="og:url" content="http://github.com/gayTao/gayTao.github.io.git/2017/05/22/Spark学学习笔记/index.html">
<meta property="og:site_name" content="桃树下的白手套">
<meta property="og:description" content="注：根据网站https://databricks.com/整理  spark核心共有4个模块，面对的实际情景如下图所示 RDD(Resilient Distribute Datasets)目前，我对spark的理解浅显的停留在一种是大数据处理的框架。那么，要处理数据，就必须了解其中重要的操作元素RDD的概念。RDD在官网的定义是：It is a fault-tolerant collection">
<meta property="og:image" content="http://i.imgur.com/YN9lQ4W.jpg">
<meta property="og:image" content="http://i.imgur.com/6DavYrm.png">
<meta property="og:updated_time" content="2017-05-26T12:35:33.565Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Spark学习笔记（1）术语概览(updating)">
<meta name="twitter:description" content="注：根据网站https://databricks.com/整理  spark核心共有4个模块，面对的实际情景如下图所示 RDD(Resilient Distribute Datasets)目前，我对spark的理解浅显的停留在一种是大数据处理的框架。那么，要处理数据，就必须了解其中重要的操作元素RDD的概念。RDD在官网的定义是：It is a fault-tolerant collection">
<meta name="twitter:image" content="http://i.imgur.com/YN9lQ4W.jpg">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Pisces',
    sidebar: {"position":"left","display":"post","offset":12,"offset_float":0,"b2t":false,"scrollpercent":false},
    fancybox: true,
    motion: true,
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://github.com/gayTao/gayTao.github.io.git/2017/05/22/Spark学学习笔记/"/>





  <title>Spark学习笔记（1）术语概览(updating) | 桃树下的白手套</title>
</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  














  
  
    
  

  <div class="container sidebar-position-left page-post-detail ">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">桃树下的白手套</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br />
            
            分类
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            归档
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            标签
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <link itemprop="mainEntityOfPage" href="http://github.com/gayTao/gayTao.github.io.git/2017/05/22/Spark学学习笔记/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="王套套">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/uploads/timg.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="桃树下的白手套">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">Spark学习笔记（1）术语概览(updating)</h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2017-05-22T17:09:06+08:00">
                2017-05-22
              </time>
            

            

            
          </span>

          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2017/05/22/Spark学学习笔记/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count disqus-comment-count"
                        data-disqus-identifier="2017/05/22/Spark学学习笔记/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          

          

          

        </div>
      </header>
    

    <div class="post-body" itemprop="articleBody">

      
      

      
        <p>注：根据网站<a href="https://databricks.com/" title="databricks" target="_blank" rel="external">https://databricks.com/</a>整理</p>
<p> spark核心共有4个模块，面对的实际情景如下图所示<br><img src="http://i.imgur.com/YN9lQ4W.jpg" alt=""></p>
<h3 id="RDD-Resilient-Distribute-Datasets"><a href="#RDD-Resilient-Distribute-Datasets" class="headerlink" title="RDD(Resilient Distribute Datasets)"></a>RDD(Resilient Distribute Datasets)</h3><p>目前，我对spark的理解浅显的停留在一种是大数据处理的框架。那么，要处理数据，就必须了解其中重要的操作元素RDD的概念。RDD在官网的定义是：It is a fault-tolerant collection of elements that can be operated on in parallel<br>.即可以并行操作的容错集合元素。它支持两种操作： transformations和actions。transformation指从原有数据按照一定规则创造出新的数据集，一个相关的典例就是常见的map操作。actions指运行完操作数据集的程序后会返回一个值，对应典例是reduce。</p>
<h3 id="SQL-DataSet和DataFrame"><a href="#SQL-DataSet和DataFrame" class="headerlink" title="SQL,DataSet和DataFrame"></a>SQL,DataSet和DataFrame</h3><p>Spark SQL 是 Spark 处理结构化数据的一个模块，功能是执行SQL查询，返回DataSet/DataFrame的查询结果。</p>
<p>一个DataSet 是一个分布式数据集，scala/java支持Dataset api，但是python不支持。</p>
<p>一个 DataFrame 所代表的是一个多个 Row（行）的 Dataset，类似关系型数据库中表的概念。Scala，java，python 和 R 均支持DataFrame api。DataFrame 可以从大量的 Source 中构造出来，像 : 结构化的数据文件，Hive中的表，外部的数据库，或者已存在的 RDD。可见，它是一种跨语言的、通用的数据科学抽象。<br><strong>总之，DataFrame 可以理解为以RDD为基础的，结构化带有模式信息的分布式数据集，具有额外的SQL接口。</strong><br>DataFrame 是按照指定列组织的分布式数据集合。</p>
<pre><code>是 SparkSQL 中的编程抽象。
支持广泛的数据格式和存储系统。
可通过 Scala、Python、Java 和 R 语言编程。
</code></pre><p>DataFrame 与 Spark RDD 的区别在于：</p>
<pre><code>DataFrame 是带有模式的集合，可以通过 SQL 语句查询。
RDD 是不具有潜在数据类型信息的对象的不透明集合，不能通过 SQL 语句查询
</code></pre><h3 id="Spark-Stream"><a href="#Spark-Stream" class="headerlink" title="Spark Stream"></a>Spark Stream</h3><p>Spark Streaming 是现有 Spark 核心 API 的一种扩展，适用于实时数据在可扩展、高吞吐、高容错等特性下的流处理。<br><img src="http://i.imgur.com/6DavYrm.png" alt=""><br>Spark Streaming 可以接收实时的数据流，然后将其划分为 Batch （可以理解为各个批次的数据块）。这些 Batch 将由真正的 Spark 引擎来处理，创建最后的处理结果的数据流，并且这些数据流也是以 Batch 的形式存在的。它们通常可以存放至 HDFS、HBase 等处。后续可以继续用 Spark 进行处理，或者是导向 Hadoop 的 MapReduce ，以及 Drill 、Hive 等是用于不同场景的数据分析工具。</p>
<h3 id="DataFrame-简单应用"><a href="#DataFrame-简单应用" class="headerlink" title="DataFrame 简单应用"></a>DataFrame 简单应用</h3><p>在shell里面输入</p>
<pre><code>spark-shell --packages com.databricks:spark-csv_2.11:1.1.0
</code></pre><p>加载完sparkshell及三方模块以后</p>
<pre><code>val df = sqlContext.read.format(&quot;com.databricks.spark.csv&quot;).option(&quot;header&quot;, &quot;true&quot;).load(&quot;/home/shiyanlou/1987.csv&quot;)
</code></pre><p>尝试取出这个数据集中的前 5 个数据，看一下是否读入成功。<br>请在 Spark Shell 中输入下面的代码。</p>
<pre><code>df.take(5)
</code></pre><p>在引入数据集之后，下一步要做的便是确保我们所有数据的格式是正确的。可以通过下面的代码来检查其格式。<br>请在 Spark Shell 中输入下面的代码。</p>
<pre><code>df.printSchema()
</code></pre><p>这里的每一项都是 String 类型。尽管这样不是很碍眼，但是在处理数据时多少都有一点不方便。我们可以尝试将某些列转换成其它的类型。方法是提取出某列然后转换。例如：<br> 请在 Spark Shell 中输入下面的代码。</p>
<pre><code>df.col(&quot;Year&quot;).cast(&quot;int&quot;)
</code></pre><p>当我们提取出某一列之后，又怎样将其放回去呢？下面我们提供一种 Spark 里改变 DataFrame 列类型的行之有效的办法。<br>请在 Spark Shell 中输入下面的代码。</p>
<pre><code>val df_1 = df.withColumnRenamed(&quot;Year&quot;,&quot;oldYear&quot;)
val df_2 = df_1.withColumn(&quot;Year&quot;,df_1.col(&quot;oldYear&quot;).cast(&quot;int&quot;)).drop(&quot;oldYear&quot;)
</code></pre><p>该想到用一个函数来转换列的格式类型。就像下面这样，请动手输入下面的代码来定义一个简单的函数：</p>
<pre><code>请在 Spark Shell 中输入下面的代码。
</code></pre><p>// 这里的类型转换参数就可以由我们自己随意决定了<br>    def convertColumn(df: org.apache.spark.sql.DataFrame, name:String, newType:String) = {<br>      val df_1 = df.withColumnRenamed(name, “swap”)<br>      df_1.withColumn(name, df_1.col(“swap”).cast(newType)).drop(“swap”)<br>    }</p>
<pre><code>val df_3 = convertColumn(df_2, &quot;ArrDelay&quot;, &quot;int&quot;)
val df_4 = convertColumn(df_2, &quot;DepDelay&quot;, &quot;int&quot;)
</code></pre><p> 计算平均延迟时间</p>
<pre><code>val averageDelays = df_4.groupBy(df_4.col(&quot;FlightNum&quot;)).agg(avg(df_4.col(&quot;ArrDelay&quot;)), avg(df_4.col(&quot;DepDelay&quot;)))
</code></pre><p>请输入下面的代码来缓存我们刚刚得到的数据。</p>
<pre><code>请在 Spark Shell 中输入下面的代码。
</code></pre><p>averageDelays.cache()</p>
<p>在缓存之后，我们如果要基于 averageDelays 做后续的计算，那么它的运算速度就会非常快了。 Spark 对于所有的变量几乎都是懒计算的，如果你不缓存的话，只有在执行行动操作（Action）时，它们才会被真正地计算。<br>下面我们来用一个行动操作看一下刚刚的计算结果。请输入这些代码。</p>
<pre><code>请在 Spark Shell 中输入下面的代码。

averageDelays.show()

averageDelays.orderBy(&quot;AVG(ArrDelay)&quot;).show()
averageDelays.sort($&quot;AVG(ArrDelay)&quot;.desc).show()
averageDelays.sort($&quot;AVG(ArrDelay)&quot;.desc, $&quot;AVG(DepDelay)&quot;.desc).show()
</code></pre><h3 id="在-Spark-中创建-DataFrame"><a href="#在-Spark-中创建-DataFrame" class="headerlink" title="在 Spark 中创建 DataFrame"></a>在 Spark 中创建 DataFrame</h3><p>有两个途径可以创建 DataFrame 。一是从现有的 RDD 创建，二是直接从数据源创建。</p>
<h4 id="从-RDD-创建-DataFrame"><a href="#从-RDD-创建-DataFrame" class="headerlink" title="从 RDD 创建 DataFrame"></a>从 RDD 创建 DataFrame</h4><p>#####（1） 由反射机制推断出模式</p>
<p>这里的模式指的就是 DataFrame 独有的模式（ Schema ）。这种方式通常适用于模式已知的情况，是将含有 case class 的 RDD 转化为 DataFrame 来实现的。</p>
<p>步骤是</p>
<h6 id="Step-1：引用必要的类"><a href="#Step-1：引用必要的类" class="headerlink" title="Step 1：引用必要的类"></a>Step 1：引用必要的类</h6><p>请在 Spark Shell 中输入下面的代码。</p>
<pre><code>import org.apache.spark.sql._

import sqlContext.implicits._
</code></pre><h6 id="Step-2：创建-RDD-。"><a href="#Step-2：创建-RDD-。" class="headerlink" title="Step 2：创建 RDD 。"></a>Step 2：创建 RDD 。</h6><p>//导入CSV文件并处理逗号分隔的数据</p>
<pre><code>val sfpdRDD = sc.textFile(&quot;/home/shiyanlou/SFPD.csv&quot;).map(inc =&gt; inc.split(&quot;,&quot;))
</code></pre><h6 id="Step-3：定义-case-class-。"><a href="#Step-3：定义-case-class-。" class="headerlink" title="Step 3：定义 case class 。"></a>Step 3：定义 case class 。</h6><pre><code>case class Incidents(incidentnum:String, category:String, description:String, dayofweek:String, date:String, time:String, pddistrict:String, resolution:String, address:String, x:String, y:String, location:String, pdid:String)
</code></pre><h6 id="Step-4：将-RDD-转换为含有-case-对象的-RDD-。"><a href="#Step-4：将-RDD-转换为含有-case-对象的-RDD-。" class="headerlink" title="Step 4：将 RDD 转换为含有 case 对象的 RDD 。"></a>Step 4：将 RDD 转换为含有 case 对象的 RDD 。</h6><pre><code>val sfpdCase = sfpdRDD.map(inc =&gt; Incidents(inc(0), inc(1), inc(2), inc(3), inc(4), inc(5), inc(6), inc(7), inc(8), inc(9), inc(10), inc(11), inc(12)))
</code></pre><h6 id="Step-5：隐式转换会将含有-case-对象的-RDD-转换为-DataFrame-，将-DataFrame-的一些操作和函数应用于这个-DataFrame-中。"><a href="#Step-5：隐式转换会将含有-case-对象的-RDD-转换为-DataFrame-，将-DataFrame-的一些操作和函数应用于这个-DataFrame-中。" class="headerlink" title="Step 5：隐式转换会将含有 case 对象的 RDD 转换为 DataFrame ，将 DataFrame 的一些操作和函数应用于这个 DataFrame 中。"></a>Step 5：隐式转换会将含有 case 对象的 RDD 转换为 DataFrame ，将 DataFrame 的一些操作和函数应用于这个 DataFrame 中。</h6><pre><code>val sfpdDF = sfpdCase.toDF()
</code></pre><h6 id="Step-6：将-DataFrame-注册为临时表，以便于在表上运行-SQL-查询"><a href="#Step-6：将-DataFrame-注册为临时表，以便于在表上运行-SQL-查询" class="headerlink" title="Step 6：将 DataFrame 注册为临时表，以便于在表上运行 SQL 查询"></a>Step 6：将 DataFrame 注册为临时表，以便于在表上运行 SQL 查询</h6><pre><code>sfpdDF.registerTempTable(&quot;sfpd&quot;)
</code></pre><p>#####（2）通过编程方式构建模式</p>
<p>这种方式适用于列和类型在运行时不可知的情况，我们就需要手动地去构建 DataFrame 的模式。通常 DataFrame 的模式在动态变化时才会使用这种方式。</p>
<h4 id="从数据源创建-DataFrame"><a href="#从数据源创建-DataFrame" class="headerlink" title="从数据源创建 DataFrame"></a>从数据源创建 DataFrame</h4><p>现有的大数据应用通常需要搜集和分析来自不同的数据源的数据。而 DataFrame 支持 JSON 文件、 Parquet 文件、 Hive 表等数据格式。它能从本地文件系统、分布式文件系统（HDFS）、云存储（Amazon S3）和外部的关系数据库系统（通过JDBC，在Spark 1.4版本起开始支持）等地方读取数据。另外，通过 Spark SQL 的外部数据源 API ，DataFrame 能够被扩展，以支持第三方的数据格式或数据源。在实验楼提供的 DataFrame 入门课程中，我们就用到了一个十分流行的第三方CSV格式扩展。除此之外，它还支持 Avro （一个数据序列化的系统）、ElasticSearch、Cassandra</p>
<p>扩展阅读：</p>
<p>CSV：    逗号分隔值（ Comma-Separated Values ），其文件以纯文本形式存储表格数据。请参考：<a href="http://commons.apache.org/proper/commons-csv/" target="_blank" rel="external">http://commons.apache.org/proper/commons-csv/</a> 。<br>    Apache Avro： 一个数据序列化的系统，相当于基于二进制数据传输高性能的中间件。请参考：<a href="https://avro.apache.org/docs/current/" target="_blank" rel="external">https://avro.apache.org/docs/current/</a> 。<br>    Elasticsearch：一个基于 Lucene 的搜索服务器。提供了一个基于 RESTful web 接口的分布式全文搜索引擎。请参考：<a href="https://www.elastic.co/products/elasticsearch/" target="_blank" rel="external">https://www.elastic.co/products/elasticsearch/</a> 。<br>    Apache Cassandra：一套开源分布式NoSQL数据库系统。请参考：<a href="http://cassandra.apache.org/" target="_blank" rel="external">http://cassandra.apache.org/</a> 。</p>
<p>// 默认格式parquet文件的加载方法，需要给出文件的路径<br>sqlContext.read.load(“/home/shiyanlou/data.parquet”)</p>
<p>// 加载其他格式的文件，需要在format方法中指明格式<br>sqlContext.read.format(“json”).load(“/home/shiyanlou/data.json”)<br>常用 DataFrame 操作</p>
<p>对于 DataFrame 而言，经常用到的行动操作（Action）主要有：</p>
<pre><code>count()：返回当前 DataFrame 中含有的行的数目。
collect()：以数组的形式，返回当前 DataFrame 中的所有行。
head()：返回当前 DataFrame 中的第一行数据。
first()：功能与 head() 相同，返回其中第一行的数据。
show()：输出 DataFrame 中的前面几行数据，默认为前 20 行，输出的数据将以表格形式呈现。
take(n:int)：返回当前 DataFrame 中前面 n 行的数据，行数由参数确定。
describe(cols:String*)：计算数值列的统计值（这些指标有计数、均值、标准差、最大值和最小值等）
</code></pre><p>除此之外，DataFrame 还有以下这些经常用到的基础函数：</p>
<pre><code>columns：以数组的形式，返回当前 DataFrame 中所有的列名。
cache()：将当前 DataFrame 缓存至内存或硬盘中（缓存方式由其他机制决定）。
persist()：持久化当前 DataFrame 。
unpersist()：与上面的持久化方法作用相反，是去除该 DataFrame 的持久化。
printSchema()：以“树目录”的形式打印表格的字段名和类型（相当于其模式）。
dtypes：以数组的形式，返回所有的列名及其数据类型。
toDF()：在重新指定列名后，返回一个新的 DataFrame 。
registerTempTable()：将指定的名字作为参数，将当前 DataFrame 注册为一个临时表。
isLocal：返回一个Boolean的结果，用于判断 collect 操作和 take 操作是否可以本地运行。
explain()：在控制台输出物理计划（Catalyst Optimizer）
</code></pre><p>由于 DataFrame 本身也是一个 RDD ，所以一些 RDD 具有的操作，DataFrame 也有。例如：</p>
<pre><code>map()
flatMap()
foreach()
repartition()
foreachPartition()
</code></pre><p>这些方法如何使用已经在 RDD 相关的课程中进行了讲解，此处不再赘述。</p>
<p>最后，我们再提供一些常用的语言集成查询函数。这些函数的作用类似于 SQL 语句，通常与 Spark SQL 结合使用。</p>
<pre><code>select()：根据参数提供的条件，选择一个列的集合。
where(condition)：根据参数提供的条件，过滤指定的行。与此作用相同的还有 filter(expr) 。
sort()：根据参数提供的表达式，返回一个按照该表达式排序的新的 DataFrame 。
drop()：移除参数指定的列，然后返回一个新的 DataFrame 。
distinct()：返回一个仅包含 DataFrame 的唯一行的新 DataFrame 。
col()：选择参数指定的列，并将其以 Column 的形式返回。
as()：以参数指定的名称作为当前 DataFrame 的别名，返回一个新的 DataFrame 。
agg(expr, exprs)：聚集整个 DataFrame 。
except(other)：返回一个带有行的新 DataFrame ，这里的行指的是当前 DataFrame 不在其他 DataFrame 中被包含的行。
groupBy(cols:Columns)：使用指定的列来分组 DataFrame 。
join(DataFrame, joinExpr)：使用指定的联合表达式，联合其他的 DataFrame 。
</code></pre><p>这些函数的参数形式、返回值类型，请通过最新的 DataFrame API 文档<a href="https://spark.apache.org/docs/1.6.1/api/scala/index.html#org.apache.spark.sql.DataFrame" title="DataFrame API文档" target="_blank" rel="external">https://spark.apache.org/docs/1.6.1/api/scala/index.html#org.apache.spark.sql.DataFrame</a>获取</p>
<pre><code>val top3Address = sqlContext.sql(&quot;SELECT address, count(incidentnum) AS inccount FROM sfpd GROUP BY address ORDER BY inccount DESC LIMIT 3&quot;).show
</code></pre><p>我们在常用 DataFrame 操作一节中讲述了行动操作、基础函数等，这里还需要再补充一下输出操作会用到的函数。输出的意思即保存，我们通常会将一些中间结果或者最终结果存储起来，作为下一步分析的基础。</p>
<p>这些用于输出的操作主要有：</p>
<pre><code>save(source, mode, options)：基于给定的数据源、保存模式和选项，将 DataFrame 的内容保存下来。
saveAsTable(tableName, source, mode, options)：基于给定的数据源、保存模式和选项，将 DataFrame 的内容保存为表。
saveAsParquetFile(path)：将 DataFrame 的内容保存为 parquet 文件，存放于指定的路径。
insertIntoJDBC(url, name, over)：将 DataFrame 的内容保存至 JDBC 源，路径由参数 url 给出，需要指定表名和覆盖模式
</code></pre><p>但我们如果需要保存为 JSON 格式，又应该怎么办呢？我们知道 JSON 文件的本质是文本，所以可通过下面的代码来实现：</p>
<p>numAddDesc.toJSON.saveAsTextFile(“/home/shiyanlou/numAddress”)<br>注意此处保存的对象 numAddDesc 是一个 DataFrame 对象。在保存完毕后，我们可以在 /home/shiyanlou 目录下看到一个名为 numAddress 的文件夹<br>在 Spark 1.4 及之后的版本中，这些保存的方法被通用的格式所取代。首先调用 write() 方法进行写操作，可以通过 format() 函数指定格式，并且通过 mode() 函数指定存储模式，用 save() 函数指定路径。于是刚刚的保存操作可以写成：</p>
<p>numAddDesc.write.format(“json”).mode(“overwrite”).save(“/home/shiyanlou/numAddDesc2”)</p>

      
    </div>

    <div>
      
        

      
    </div>

    <div>
      
        

      
    </div>

    <div>
      
        

      
    </div>

    <footer class="post-footer">
      

      
        
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2017/05/22/hadoop学习笔记-1-单机环境搭建与词频统计/" rel="next" title="hadoop学习笔记(1)单机环境搭建与词频统计">
                <i class="fa fa-chevron-left"></i> hadoop学习笔记(1)单机环境搭建与词频统计
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2017/05/25/linux常见命令-updating/" rel="prev" title="linux常见命令(updating)">
                linux常见命令(updating) <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          
  <div class="comments" id="comments">
    
      <div id="disqus_thread">
        <noscript>
          Please enable JavaScript to view the
          <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a>
        </noscript>
      </div>
    
  </div>


        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap" >
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview">
            站点概览
          </li>
        </ul>
      

      <section class="site-overview sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
          <img class="site-author-image" itemprop="image"
               src="/uploads/timg.jpg"
               alt="王套套" />
          <p class="site-author-name" itemprop="name">王套套</p>
           
              <p class="site-description motion-element" itemprop="description">每个人的心底都有一座坟墓，是用来埋葬所爱的人的</p>
          
        </div>
        <nav class="site-state motion-element">

          
            <div class="site-state-item site-state-posts">
              <a href="/archives">
                <span class="site-state-item-count">10</span>
                <span class="site-state-item-name">日志</span>
              </a>
            </div>
          

          

          
            
            
            <div class="site-state-item site-state-tags">
              <a href="/tags/index.html">
                <span class="site-state-item-count">3</span>
                <span class="site-state-item-name">标签</span>
              </a>
            </div>
          

        </nav>

        

        <div class="links-of-author motion-element">
          
            
              <span class="links-of-author-item">
                <a href="https://github.com/gayTao" target="_blank" title="GitHub">
                  
                    <i class="fa fa-fw fa-github"></i>
                  
                  GitHub
                </a>
              </span>
            
              <span class="links-of-author-item">
                <a href="http://www.zhihu.com/people/wei-ting-72?utm_source=qq&utm_medium=social" target="_blank" title="知乎">
                  
                    <i class="fa fa-fw fa-globe"></i>
                  
                  知乎
                </a>
              </span>
            
          
        </div>

        
        

        
        

        


      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-3"><a class="nav-link" href="#RDD-Resilient-Distribute-Datasets"><span class="nav-number">1.</span> <span class="nav-text">RDD(Resilient Distribute Datasets)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#SQL-DataSet和DataFrame"><span class="nav-number">2.</span> <span class="nav-text">SQL,DataSet和DataFrame</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Spark-Stream"><span class="nav-number">3.</span> <span class="nav-text">Spark Stream</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#DataFrame-简单应用"><span class="nav-number">4.</span> <span class="nav-text">DataFrame 简单应用</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#在-Spark-中创建-DataFrame"><span class="nav-number">5.</span> <span class="nav-text">在 Spark 中创建 DataFrame</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#从-RDD-创建-DataFrame"><span class="nav-number">5.1.</span> <span class="nav-text">从 RDD 创建 DataFrame</span></a><ol class="nav-child"><li class="nav-item nav-level-6"><a class="nav-link" href="#Step-1：引用必要的类"><span class="nav-number">5.1.0.1.</span> <span class="nav-text">Step 1：引用必要的类</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#Step-2：创建-RDD-。"><span class="nav-number">5.1.0.2.</span> <span class="nav-text">Step 2：创建 RDD 。</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#Step-3：定义-case-class-。"><span class="nav-number">5.1.0.3.</span> <span class="nav-text">Step 3：定义 case class 。</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#Step-4：将-RDD-转换为含有-case-对象的-RDD-。"><span class="nav-number">5.1.0.4.</span> <span class="nav-text">Step 4：将 RDD 转换为含有 case 对象的 RDD 。</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#Step-5：隐式转换会将含有-case-对象的-RDD-转换为-DataFrame-，将-DataFrame-的一些操作和函数应用于这个-DataFrame-中。"><span class="nav-number">5.1.0.5.</span> <span class="nav-text">Step 5：隐式转换会将含有 case 对象的 RDD 转换为 DataFrame ，将 DataFrame 的一些操作和函数应用于这个 DataFrame 中。</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#Step-6：将-DataFrame-注册为临时表，以便于在表上运行-SQL-查询"><span class="nav-number">5.1.0.6.</span> <span class="nav-text">Step 6：将 DataFrame 注册为临时表，以便于在表上运行 SQL 查询</span></a></li></ol></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#从数据源创建-DataFrame"><span class="nav-number">5.2.</span> <span class="nav-text">从数据源创建 DataFrame</span></a></li></ol></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright" >
  
  &copy; 
  <span itemprop="copyrightYear">2017</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">王套套</span>
</div>


<div class="powered-by">
  由 <a class="theme-link" href="https://hexo.io">Hexo</a> 强力驱动
</div>

<div class="theme-info">
  主题 -
  <a class="theme-link" href="https://github.com/iissnan/hexo-theme-next">
    NexT.Pisces
  </a>
</div>


        

        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>

  
  <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>

  
  <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.1"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.1"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=5.1.1"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.1"></script>



  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.1"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.1"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.1"></script>



  


  

    
      <script id="dsq-count-scr" src="https://.disqus.com/count.js" async></script>
    

    
      <script type="text/javascript">
        var disqus_config = function () {
          this.page.url = 'http://github.com/gayTao/gayTao.github.io.git/2017/05/22/Spark学学习笔记/';
          this.page.identifier = '2017/05/22/Spark学学习笔记/';
          this.page.title = 'Spark学习笔记（1）术语概览(updating)';
        };
        var d = document, s = d.createElement('script');
        s.src = 'https://.disqus.com/embed.js';
        s.setAttribute('data-timestamp', '' + +new Date());
        (d.head || d.body).appendChild(s);
      </script>
    

  




	





  





  





  






  





  

  

  

  

  

</body>
</html>
